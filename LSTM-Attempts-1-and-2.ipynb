{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Attempt 1 - Akira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import spacy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_loaders(df, embedding_type, batch_size=600, test_ratio=0.9):\n",
    "  max_row=96562\n",
    "  if embedding_type==\"tfidf\":\n",
    "    embedding=np.array([r.toarray() for r in train[\"tfidf\"].values[:max_row]]).reshape((max_row, 100))\n",
    "  elif embedding_type==\"glove\":\n",
    "    embedding=np.array([np.array(r)[:-1] for r in train[\"glove\"].values[:max_row]])\n",
    "  elif embedding_type==\"bert\":\n",
    "    embedding=np.array([np.array(r) for r in train[\"bert\"].values[:max_row]])\n",
    "  elif embedding_type==\"d2v\":\n",
    "    embedding=np.array([np.array(r) for r in train[\"d2v\"].values[:max_row]])\n",
    "  elif embedding_type==\"use\":\n",
    "    embedding=np.array([np.array(r) for r in train[\"use\"].values[:max_row]]).reshape((max_row, 512))\n",
    "  elif embedding_type==\"cv\":\n",
    "    embedding=np.array([r.toarray() for r in train[\"cv\"].values[:max_row]]).reshape((max_row, 100))\n",
    "  else:\n",
    "    print(\"embedding type not supported\")\n",
    "    return -1\n",
    "\n",
    "  test_index=int(max_row*test_ratio)\n",
    "  at_count=np.array(df[\"@_count\"].values[:max_row]).reshape((-1,1))\n",
    "  hash_count=np.array(df[\"#_count\"].values[:max_row]).reshape((-1,1))\n",
    "  year=np.array(df[\"year\"].values[:max_row]).reshape((-1,1))\n",
    "  month=np.array(df[\"month\"].values[:max_row]).reshape((-1,1))\n",
    "  week_day=np.array(df[\"week_day\"].values[:max_row]).reshape((-1,1))\n",
    "  hour=np.array(df[\"hour\"].values[:max_row]).reshape((-1,1))\n",
    "  #is_reply_to_tweet=np.array(df[\"is_reply_to_tweet\"].values[:max_row]).reshape((-1,1))\n",
    "  #is_reply_to_user=np.array(df[\"is_reply_to_user\"].values[:max_row]).reshape((-1,1))\n",
    "  \n",
    "  so_far=np.array(df[\"user_total_retweeted_so_far\"].values[:max_row]).reshape((-1,1))\n",
    "\n",
    "  \n",
    "  #user_id=np.array(df[\"user_id\"].values[:max_row]).reshape((-1,1))\n",
    "  url_count=np.array(df[\"url_count\"].values[:max_row]).reshape((-1,1))\n",
    "  labels=np.array(df[\"retweet_count\"].values[:max_row]).reshape((-1,1)).astype(np.float32)\n",
    "  features=np.hstack(( embedding, at_count, hash_count, year, month, week_day, hour, url_count, so_far)).astype(np.float32)\n",
    "  print(np.max(labels))\n",
    "  #np.random.shuffle(features)\n",
    "  #labels=features[:,:1]\n",
    "  #features=features[:,1:]\n",
    "  #features=features.reshape((features.shape[0],1,features.shape[1]))\n",
    "\n",
    "  train_data = TensorDataset(torch.from_numpy(features[:test_index]), torch.from_numpy(labels[:test_index]))\n",
    "  test_data = TensorDataset(torch.from_numpy(features[test_index:]), torch.from_numpy(labels[test_index:]))\n",
    "  print(features)\n",
    "  train_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size)\n",
    "  test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "  return train_loader, test_loader, features.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retweet_Net(nn.Module):\n",
    "    def __init__(self, feature_num, hidden_units=512):\n",
    "        super().__init__()\n",
    "        self.feature_num = feature_num\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = 2\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=feature_num,\n",
    "            hidden_size=hidden_units,\n",
    "            batch_first=True,\n",
    "            num_layers=self.num_layers,\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(in_features=self.hidden_units, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #self.lstm.flatten_parameters()\n",
    "        #_, (hidden, _) = self.lstm(x)\n",
    "        #out = hidden[-1]\n",
    "        batch_size = x.shape[1]\n",
    "        h0 = torch.zeros(self.num_layers, self.hidden_units).requires_grad_().cuda()\n",
    "        c0 = torch.zeros(self.num_layers, self.hidden_units).requires_grad_().cuda()\n",
    "\n",
    "        _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        out = self.linear(hn[0]).flatten()\n",
    "\n",
    "        return nn.functional.relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_loader, model, loss_function, optimizer, batch_size=600, epochs=10):\n",
    "    for e in range(epochs):\n",
    "        print(\"Epoch: \"+str(e+1)+\"/\"+str(epochs))\n",
    "        num_batches = len(data_loader)\n",
    "        total_loss = []\n",
    "        num_of_exception_loss=0\n",
    "        total_out=[]\n",
    "        total_y=[]\n",
    "        max_loss=0\n",
    "        for index, (X, y) in enumerate(data_loader):\n",
    "            model.train()\n",
    "            X=X.cuda()\n",
    "            y=y.cuda()\n",
    "            output = model(X)\n",
    "            loss = loss_function(output, y)\n",
    "            \n",
    "            if index%2000==0:\n",
    "              print(index)\n",
    "              print(output)\n",
    "              print(y)\n",
    "              print(loss)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss.append(loss.item())\n",
    "            total_y.append(np.mean(y.cpu().detach().numpy().T))\n",
    "            if loss.item()>10000:\n",
    "              #total_loss.pop()\n",
    "              num_of_exception_loss+=1\n",
    "            total_out.append(output.item())\n",
    "            \n",
    "\n",
    "        plt.scatter(list(range(len(total_loss))), total_loss)\n",
    "        plt.show()\n",
    "        plt.scatter(list(range(len(total_out))), total_out)\n",
    "        plt.show()\n",
    "        plt.scatter(list(range(len(total_y))), total_y)\n",
    "        plt.show()\n",
    "        avg_loss = sum(total_loss) / len(total_loss)\n",
    "        print(f\"Train loss: {avg_loss}\", \"exception: \"+str(num_of_exception_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(data_loader, model, loss_function1):\n",
    "    loss_function = nn.MSELoss()\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = []\n",
    "    num_of_exception_loss=0\n",
    "    total_out=[]\n",
    "    model.eval()\n",
    "    total_y=[]\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X=X.cuda()\n",
    "            y=y.cuda()\n",
    "            output = model(X)\n",
    "            loss=loss_function(output, y)\n",
    "            total_loss.append(loss.item())\n",
    "            if loss.item()>10000:\n",
    "               # total_loss.pop()\n",
    "                num_of_exception_loss+=1\n",
    "            total_out.append(output.item())\n",
    "            total_y.append(np.mean(y.cpu().detach().numpy().T))\n",
    "\n",
    "    plt.scatter(list(range(len(total_loss))), total_loss)\n",
    "    plt.show()\n",
    "    plt.scatter(list(range(len(total_out))), total_out)\n",
    "    plt.show()\n",
    "    plt.scatter(list(range(len(total_y))), total_y)\n",
    "    plt.show()\n",
    "    avg_loss = sum(total_loss) / len(total_loss)\n",
    "    print(f\"Train loss mse: {avg_loss}\", \"exception: \"+str(num_of_exception_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with glove embedding first\n",
    "batch_size=10\n",
    "train_glove, test_glove, feature_num = create_train_test_loaders(train, \"glove\", batch_size=batch_size, test_ratio=0.8)\n",
    "print(len(train_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Retweet_Net(feature_num)\n",
    "model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "#loss_function=nn.PoissonNLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(train_glove,model, loss_function, optimizer, batch_size, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(test_glove, model, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Attempt 2 - Akira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zhuyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ORIGINAL: https://github.com/sagarjinde/Tweet-Popularity-Prediction\n",
    "\n",
    "# Basic packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import regex\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Packages for data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# Packages for modeling\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Embedding, LSTM, Input, SimpleRNN, TimeDistributed, Concatenate, BatchNormalization, LeakyReLU\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\n",
    "VAL_SIZE = 1000  # Size of the validation set\n",
    "NB_START_EPOCHS = 10  # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\n",
    "MAX_LEN = 26  # Maximum number of words in a sequence\n",
    "GLOVE_DIM = 100  # Number of dimensions of the GloVe word embeddings\n",
    "LSTM_OUT = 256  # output dimension of language model lstm\n",
    "NORMALIZE_TO = 10  # normalize the value of features between 0 to NORMALIZE_TO\n",
    "RETWEETS_NORM_TO = 10    # normalize retweet between 0 to RETWEETS_NORM_TO\n",
    "HOURS = 2  # number of hours the dataset was recorded for\n",
    "RANDOM_NUM = random.randint(0,100)\n",
    "retweet_count_hour_path=\"./retweet_count_\"+str(HOURS)+\".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./train.json\", 'r') as j:\n",
    "     train = json.loads(j.read())\n",
    "        \n",
    "custom_columns=[\"tweet_id\",\"text\",\"timestamp\",\"year\",\"month\",\"week_day\",\"hour\",\"is_reply_to_tweet\",\"is_reply_to_user\",\n",
    "            \"user_id\",\"@_count\",\"#_count\",\"url_count\",\"tweet_len\", \"rewteet_count\", \"favorite_count\", \n",
    "                'friends_count', 'followers_count', 'account_age', 'total_tweet_count', 'favourited_tweet_count']\n",
    "features=[]\n",
    "for i,twt in enumerate(train):\n",
    "    row=[twt[\"id_str\"], twt[\"text\"]]\n",
    "    time=parse(twt['created_at'])\n",
    "    row.extend([int(time.timestamp()), time.year, time.month, time.weekday(), time.hour])\n",
    "    if twt[\"in_reply_to_status_id_str\"]==None:\n",
    "        row.append(0)\n",
    "    else:\n",
    "        row.append(1)\n",
    "\n",
    "    if twt[\"in_reply_to_user_id_str\"]==None:\n",
    "        row.append(0)\n",
    "    else:\n",
    "        row.append(1)\n",
    "\n",
    "    row.append(twt[\"user\"][\"id_str\"])\n",
    "    row.append(len(twt[\"entities\"][\"user_mentions\"]))\n",
    "    row.append(len(twt[\"entities\"][\"hashtags\"]))\n",
    "    row.append(len(twt[\"entities\"][\"urls\"]))\n",
    "    row.append(len(twt[\"text\"]))\n",
    "    row.append(twt[\"retweet_count\"])\n",
    "    row.append(twt[\"favorite_count\"])\n",
    "    row.append(twt[\"user\"][\"friends_count\"])\n",
    "    row.append(twt[\"user\"][\"followers_count\"])\n",
    "    age=2022-parse(twt[\"user\"]['created_at']).year\n",
    "    row.append(age)\n",
    "    row.append(twt[\"user\"][\"listed_count\"])\n",
    "    row.append(twt[\"user\"][\"favourites_count\"])\n",
    "\n",
    "    features.append(row)\n",
    "\n",
    "feature_df=pd.DataFrame(data=features, columns=custom_columns)\n",
    "feature_df.to_csv(\"./processed_X.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved retweet_count_2.csv\n"
     ]
    }
   ],
   "source": [
    "def generate_retweet_count_time_csv(train, hours=72):\n",
    "    hours=hours+1\n",
    "    columns_hours=[\"tweet_id\"]+list(range(hours))\n",
    "    retweet_count=[]\n",
    "    for i,twt in enumerate(train):\n",
    "        row=[twt[\"id_str\"]]\n",
    "        rt_c=twt[\"retweet_count\"]\n",
    "        if hours>1:\n",
    "            row.append(0)\n",
    "            step=rt_c/(hours-1)\n",
    "            for j in range(1, hours):\n",
    "                row.append(int(step*j))\n",
    "        else:\n",
    "            row.append(rt_c)\n",
    "        retweet_count.append(row)\n",
    "    retweet_count_df=pd.DataFrame(data=retweet_count, columns=columns_hours)\n",
    "    retweet_count_df.to_csv(\"./retweet_count_\"+str(hours-1)+\".csv\",index=False)\n",
    "    print(\"saved \"+\"retweet_count_\"+str(hours-1)+\".csv\")\n",
    "    return\n",
    "\n",
    "#Make sure that hours number matches the HOURS constant or at least a csv of that hours exist\n",
    "generate_retweet_count_time_csv(train, HOURS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "\"\"\"Tweet text pre-processing\"\"\"\n",
    "\n",
    "FLAGS = re.MULTILINE | re.DOTALL | regex.VERSION1\n",
    "\n",
    "def hashtag(text):\n",
    "    text = text.group()\n",
    "    hashtag_body = text[1:]\n",
    "    if hashtag_body.isupper():\n",
    "        result = \"<hashtag> {} <allcaps>\".format(hashtag_body)\n",
    "    else:\n",
    "        result = \" \".join([\"<hashtag>\"] + regex.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
    "    return result\n",
    "\n",
    "def allcaps(text):\n",
    "    text = text.group()\n",
    "    return text.lower() + \" <allcaps>\"\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "    text = re_sub(r\"<3\",\"<heart>\")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "    text = re_sub(r\"#\\S+\", hashtag)\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "\n",
    "    ## -- I just don't understand why the Ruby script adds <allcaps> to everything so I limited the selection.\n",
    "    # text = re_sub(r\"([^a-z0-9()<>'`\\-]){2,}\", allcaps)\n",
    "    text = re_sub(r\"([A-Z]){2,}\", allcaps)\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "def deep_model(model, X_train, u_train, a_train, y_train, X_valid, u_valid, a_valid, y_valid):\n",
    "\n",
    "    train_shape = a_train.shape\n",
    "    train_shape = list(train_shape)\n",
    "    train_shape.append(1)\n",
    "\n",
    "    valid_shape = a_valid.shape\n",
    "    valid_shape = list(valid_shape)\n",
    "    valid_shape.append(1)\n",
    "\n",
    "    history = model.fit([X_train, u_train, a_train.values.reshape(train_shape)]\n",
    "                       , y_train.values.reshape(train_shape)\n",
    "                       , epochs=NB_START_EPOCHS\n",
    "                       , batch_size=BATCH_SIZE\n",
    "                       , validation_data=([X_valid, u_valid, a_valid.values.reshape(valid_shape)]\n",
    "                                          , y_valid.values.reshape(valid_shape))\n",
    "                       , verbose=1\n",
    "                       , shuffle=True)\n",
    "\n",
    "    return history\n",
    "\n",
    "def remove_stopwords(input_text):\n",
    "    '''\n",
    "    Function to remove English stopwords from a Pandas Series.\n",
    "    \n",
    "    Parameters:\n",
    "        input_text : text to clean\n",
    "    Output:\n",
    "        cleaned Pandas Series \n",
    "    '''\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "    return \" \".join(clean_words) \n",
    "    \n",
    "def remove_mentions(input_text):\n",
    "    '''\n",
    "    Function to remove mentions, preceded by @, in a Pandas Series\n",
    "    \n",
    "    Parameters:\n",
    "        input_text : text to clean\n",
    "    Output:\n",
    "        cleaned Pandas Series \n",
    "    '''\n",
    "    return re.sub(r'@\\w+', '', input_text)\n",
    "\n",
    "def eval_metric(history, metric_name):\n",
    "    '''\n",
    "    Function to evaluate a trained model on a chosen metric. \n",
    "    Training and validation metric are plotted in a\n",
    "    line chart for each epoch.\n",
    "    \n",
    "    Parameters:\n",
    "        history : model training history\n",
    "        metric_name : loss or accuracy\n",
    "    Output:\n",
    "        line chart with epochs of x-axis and metric on\n",
    "        y-axis\n",
    "    '''\n",
    "    metric = history.history[metric_name]\n",
    "    val_metric = history.history['val_' + metric_name]\n",
    "\n",
    "    e = range(1, NB_START_EPOCHS + 1)\n",
    "\n",
    "    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n",
    "    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def test_model(model, X_train, y_train, X_test, y_test, epoch_stop):\n",
    "    '''\n",
    "    Function to test the model on new data after training it\n",
    "    on the full training data with the optimal number of epochs.\n",
    "    \n",
    "    Parameters:\n",
    "        model : trained model\n",
    "        X_train : training features\n",
    "        y_train : training target\n",
    "        X_test : test features\n",
    "        y_test : test target\n",
    "        epochs : optimal number of epochs\n",
    "    Output:\n",
    "        test accuracy and test loss\n",
    "    '''\n",
    "    model.fit(X_train\n",
    "              , y_train\n",
    "              , epochs=epoch_stop\n",
    "              , batch_size=BATCH_SIZE\n",
    "              , verbose=0)\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def poisson_loss(y_actual, y_predicted):\n",
    "    loss = K.exp(y_predicted) - y_actual*y_predicted\n",
    "    return loss\n",
    "\n",
    "def save_model(model):\n",
    "    model.save('./saved_models/final_model.h5')\n",
    "\n",
    "def get_model():\n",
    "    model = models.load_model('./saved_models/final_model.h5', custom_objects={'poisson_loss': poisson_loss})\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX RETWEET COUNT:  34655\n",
      "# Train data samples: 86905\n",
      "# Test data samples: 9657\n",
      "# Train data samples: 86905\n",
      "# Test data samples: 9657\n",
      "MAX_LEN:  42\n",
      "Shape of training set: (78214, 42)\n",
      "Shape of validation set: (8691, 42)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./processed_X.csv') \n",
    "df = df[['tweet_id', 'text', 'friends_count', 'followers_count', 'account_age', 'total_tweet_count', 'favourited_tweet_count']]      # X, y\n",
    "df.text = df.text.apply(tokenize).apply(remove_stopwords).apply(remove_mentions)\n",
    "\n",
    "# find maximum\n",
    "dg = pd.read_csv(retweet_count_hour_path)\n",
    "dg = dg[[str(HOURS)]]\n",
    "max_retweet_count = int(dg.max())\n",
    "print('MAX RETWEET COUNT: ',max_retweet_count)\n",
    "\n",
    "# normalize data for hours 0 to 71 (auxilary hours)\n",
    "hrs = [str(i) for i in range(HOURS)]\n",
    "dg = pd.read_csv(retweet_count_hour_path)\n",
    "dg = (dg[hrs]/max_retweet_count)*RETWEETS_NORM_TO\n",
    "# dg = dg[hrs]\n",
    "\n",
    "X_train, X_test, a_train, a_test = train_test_split(df.text, dg, test_size=0.1, random_state=RANDOM_NUM)\n",
    "print('# Train data samples:', X_train.shape[0])\n",
    "print('# Test data samples:', X_test.shape[0])\n",
    "assert X_train.shape[0] == a_train.shape[0]\n",
    "assert X_test.shape[0] == a_test.shape[0]\n",
    "\n",
    "# normalize data for hours 1 to 72 (output hours)\n",
    "hrs = [str(i) for i in range(1,HOURS+1)]\n",
    "dg = pd.read_csv(retweet_count_hour_path)\n",
    "dg = (dg[hrs]/max_retweet_count)*RETWEETS_NORM_TO\n",
    "# dg = dg[hrs]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.text, dg, test_size=0.1, random_state=RANDOM_NUM)\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]\n",
    "\n",
    "# user account features\n",
    "dh = pd.read_csv('./processed_X.csv') \n",
    "user_featuers = ['friends_count', 'followers_count', 'account_age', 'total_tweet_count', 'favourited_tweet_count']\n",
    "\n",
    "features = []\n",
    "user_feature_count = len(user_featuers)\n",
    "for i in range(user_feature_count-1):\n",
    "    for j in range(i+1,user_feature_count):\n",
    "        feature = dh[user_featuers[i]]*dh[user_featuers[j]]\n",
    "        max_feature_value = feature.max()\n",
    "        feature = (feature/max_feature_value)*NORMALIZE_TO\n",
    "        features.append(feature)\n",
    "\n",
    "features_count = len(features)\n",
    "features = pd.concat(features, axis=1)\n",
    "\n",
    "X_train, X_test, u_train, u_test = train_test_split(df.text, features, test_size=0.1, random_state=RANDOM_NUM)\n",
    "print('# Train data samples:', X_train.shape[0])\n",
    "print('# Test data samples:', X_test.shape[0])\n",
    "assert X_train.shape[0] == u_train.shape[0]\n",
    "assert X_test.shape[0] == u_test.shape[0]\n",
    "\n",
    "seq_lengths = X_train.apply(lambda x: len(x.split(' ')))\n",
    "tweet_stats = seq_lengths.describe()\n",
    "MAX_LEN = int(tweet_stats['max'])\n",
    "print('MAX_LEN: ',MAX_LEN)\n",
    "\n",
    "\n",
    "tk = Tokenizer(num_words=NB_WORDS,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True,\n",
    "               split=\" \")\n",
    "tk.fit_on_texts(X_train)      # creates a internal dictionary\n",
    "X_train_seq = tk.texts_to_sequences(X_train)\n",
    "X_test_seq = tk.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_seq_trunc = pad_sequences(X_train_seq, maxlen=MAX_LEN)\n",
    "X_test_seq_trunc = pad_sequences(X_test_seq, maxlen=MAX_LEN)\n",
    "\n",
    "X_train_emb, X_valid_emb, y_train_emb, y_valid_emb = train_test_split(X_train_seq_trunc, y_train, test_size=0.1, random_state=RANDOM_NUM)\n",
    "X_train_emb, X_valid_emb, a_train_emb, a_valid_emb = train_test_split(X_train_seq_trunc, a_train, test_size=0.1, random_state=RANDOM_NUM)\n",
    "X_train_emb, X_valid_emb, u_train_emb, u_valid_emb = train_test_split(X_train_seq_trunc, u_train, test_size=0.1, random_state=RANDOM_NUM)\n",
    "assert X_valid_emb.shape[0] == y_valid_emb.shape[0]\n",
    "assert X_train_emb.shape[0] == y_train_emb.shape[0]\n",
    "print('Shape of training set:',X_train_emb.shape)\n",
    "print('Shape of validation set:',X_valid_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating Dictionary...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Creating a Dictionary\"\"\"\n",
    "print('creating Dictionary...')\n",
    "glove_file = './glove.twitter.27B.100d.txt'\n",
    "emb_dict = {}\n",
    "glove = open(glove_file, encoding=\"utf8\")\n",
    "for line in glove:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    emb_dict[word] = vector\n",
    "glove.close()\n",
    "\n",
    "emb_matrix = np.zeros((NB_WORDS, GLOVE_DIM))\n",
    "for w, i in tk.word_index.items():\n",
    "    # The word_index contains a token for all words of the training data so we need to limit that\n",
    "    if i < NB_WORDS:\n",
    "        vect = emb_dict.get(w)\n",
    "        # Check if the word from the training data occurs in the GloVe word embeddings\n",
    "        # Otherwise the vector is kept with only zeros\n",
    "        if vect is not None:\n",
    "            emb_matrix[i] = vect\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 42)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 42, 100)      1000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          365568      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 266)          0           lstm_1[0][0]                     \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, None, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          34176       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "rnn_1 (SimpleRNN)               [(None, None, 128),  16640       input_3[0][0]                    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 1)      129         rnn_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 1,416,513\n",
      "Trainable params: 416,513\n",
      "Non-trainable params: 1,000,000\n",
      "__________________________________________________________________________________________________\n",
      "start train\n",
      "Epoch 1/10\n",
      "153/153 [==============================] - 248s 2s/step - loss: 2.3164 - val_loss: 0.0188\n",
      "Epoch 2/10\n",
      "153/153 [==============================] - 270s 2s/step - loss: 0.1350 - val_loss: 0.0127\n",
      "Epoch 3/10\n",
      "153/153 [==============================] - 274s 2s/step - loss: 0.0069 - val_loss: 0.0128\n",
      "Epoch 4/10\n",
      "153/153 [==============================] - 270s 2s/step - loss: 0.0077 - val_loss: 0.0128\n",
      "Epoch 5/10\n",
      "153/153 [==============================] - 275s 2s/step - loss: 0.0071 - val_loss: 0.0127\n",
      "Epoch 6/10\n",
      "153/153 [==============================] - 274s 2s/step - loss: 0.0072 - val_loss: 0.0129\n",
      "Epoch 7/10\n",
      "153/153 [==============================] - 271s 2s/step - loss: 0.0072 - val_loss: 0.0129\n",
      "Epoch 8/10\n",
      "153/153 [==============================] - 273s 2s/step - loss: 0.0067 - val_loss: 0.0126\n",
      "Epoch 9/10\n",
      "153/153 [==============================] - 278s 2s/step - loss: 0.0072 - val_loss: 0.0129\n",
      "Epoch 10/10\n",
      "153/153 [==============================] - 282s 2s/step - loss: 0.0070 - val_loss: 0.0126\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building Model\"\"\"\n",
    "\n",
    "# This is for training\n",
    "encoder_inputs = Input(shape=(MAX_LEN, ), name='input_1')\n",
    "embedding = Embedding(NB_WORDS, GLOVE_DIM, name='embedding_1')\n",
    "embedding_inputs = embedding(encoder_inputs)\n",
    "encoder = LSTM(LSTM_OUT, recurrent_dropout = 0.3, dropout = 0.3, name='lstm_1', kernel_regularizer=regularizers.l2(0.05))\n",
    "lstm_output = encoder(embedding_inputs)\n",
    "user_info_inputs = Input(shape=(features_count,), name='input_2')\n",
    "dense1 = Dense(units=128, name='dense_1', kernel_regularizer=regularizers.l2(0.05))     # Whc\n",
    "full_info = Concatenate(name='concatenate_1')([lstm_output, user_info_inputs])\n",
    "encoder_output_dense1 = dense1(full_info)\n",
    "\n",
    "decoder_inputs = Input(shape=(None, 1), name='input_3')\n",
    "\n",
    "# dynamic RNN\n",
    "decoder_rnn = SimpleRNN(128, return_sequences=True, return_state=True, name='rnn_1', kernel_regularizer=regularizers.l2(0.05))\n",
    "time_distributed = TimeDistributed(Dense(1, activation='linear'), name='time_distributed_1')\n",
    "\n",
    "# decoder_outputs \n",
    "# We are passing encoder_output as the hidden state of dynamic RNN\n",
    "decoder_outputs, _ = decoder_rnn(decoder_inputs, initial_state=encoder_output_dense1)\n",
    "decoder_outputs = time_distributed(decoder_outputs)\n",
    "final_model = Model([encoder_inputs, user_info_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# weights of encoder is already there in decoder, hence we dont need to call it saperatly.\n",
    "# final_model.load_weights(path+'/saved_models/final_model.h5', by_name=True)\n",
    "\n",
    "# This is what makes model use pre-trained weights\n",
    "final_model.layers[1].set_weights([emb_matrix])\n",
    "final_model.layers[1].trainable = False\n",
    "opt = opt = optimizers.Adam(learning_rate=0.01, clipnorm=1.0)\n",
    "final_model.compile(optimizer=opt, loss=poisson_loss)\n",
    "final_model.summary()\n",
    "\n",
    "print(\"start train\")\n",
    "\n",
    "glove_history = deep_model(final_model, X_train_emb, u_train_emb, a_train_emb, y_train_emb, X_valid_emb, u_valid_emb, a_valid_emb, y_valid_emb)\n",
    "\n",
    "save_model(final_model)\n",
    "\n",
    "with open('./loss_log_total.txt', 'w') as f:\n",
    "    for key, value in glove_history.history.items():\n",
    "        f.write('%s:%s\\n' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 42)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 42, 100)      1000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          365568      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 266)          0           lstm_1[0][0]                     \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          34176       concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,399,744\n",
      "Trainable params: 399,744\n",
      "Non-trainable params: 1,000,000\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, None, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "rnn_1 (SimpleRNN)               [(None, None, 128),  16640       input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 1)      129         rnn_1[2][0]                      \n",
      "==================================================================================================\n",
      "Total params: 16,769\n",
      "Trainable params: 16,769\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "test: 1/9657\n",
      "test: 2/9657\n",
      "test: 3/9657\n",
      "test: 4/9657\n",
      "test: 5/9657\n",
      "test: 6/9657\n",
      "test: 7/9657\n",
      "test: 8/9657\n",
      "test: 9/9657\n",
      "test: 10/9657\n",
      "test: 11/9657\n",
      "test: 12/9657\n",
      "test: 13/9657\n",
      "test: 14/9657\n",
      "test: 15/9657\n",
      "test: 16/9657\n",
      "test: 17/9657\n",
      "test: 18/9657\n",
      "test: 19/9657\n",
      "test: 20/9657\n",
      "test: 21/9657\n",
      "test: 22/9657\n",
      "test: 23/9657\n",
      "test: 24/9657\n",
      "test: 25/9657\n",
      "test: 26/9657\n",
      "test: 27/9657\n",
      "test: 28/9657\n",
      "test: 29/9657\n",
      "test: 30/9657\n",
      "test: 31/9657\n",
      "test: 32/9657\n",
      "test: 33/9657\n",
      "test: 34/9657\n",
      "test: 35/9657\n",
      "test: 36/9657\n",
      "test: 37/9657\n",
      "test: 38/9657\n",
      "test: 39/9657\n",
      "test: 40/9657\n",
      "test: 41/9657\n",
      "test: 42/9657\n",
      "test: 43/9657\n",
      "test: 44/9657\n",
      "test: 45/9657\n",
      "test: 46/9657\n",
      "test: 47/9657\n",
      "test: 48/9657\n",
      "test: 49/9657\n",
      "test: 50/9657\n",
      "test: 51/9657\n",
      "test: 52/9657\n",
      "test: 53/9657\n",
      "test: 54/9657\n",
      "test: 55/9657\n",
      "test: 56/9657\n",
      "test: 57/9657\n",
      "test: 58/9657\n",
      "test: 59/9657\n",
      "test: 60/9657\n",
      "test: 61/9657\n",
      "test: 62/9657\n",
      "test: 63/9657\n",
      "test: 64/9657\n",
      "test: 65/9657\n",
      "test: 66/9657\n",
      "test: 67/9657\n",
      "test: 68/9657\n",
      "test: 69/9657\n",
      "test: 70/9657\n",
      "test: 71/9657\n",
      "test: 72/9657\n",
      "test: 73/9657\n",
      "test: 74/9657\n",
      "test: 75/9657\n",
      "test: 76/9657\n",
      "test: 77/9657\n",
      "test: 78/9657\n",
      "test: 79/9657\n",
      "test: 80/9657\n",
      "test: 81/9657\n",
      "test: 82/9657\n",
      "test: 83/9657\n",
      "test: 84/9657\n",
      "test: 85/9657\n",
      "test: 86/9657\n",
      "test: 87/9657\n",
      "test: 88/9657\n",
      "test: 89/9657\n",
      "test: 90/9657\n",
      "test: 91/9657\n",
      "test: 92/9657\n",
      "test: 93/9657\n",
      "test: 94/9657\n",
      "test: 95/9657\n",
      "test: 96/9657\n",
      "test: 97/9657\n",
      "test: 98/9657\n",
      "test: 99/9657\n",
      "test: 100/9657\n",
      "test: 101/9657\n",
      "test: 102/9657\n",
      "test: 103/9657\n",
      "test: 104/9657\n",
      "test: 105/9657\n",
      "test: 106/9657\n",
      "test: 107/9657\n",
      "test: 108/9657\n",
      "test: 109/9657\n",
      "test: 110/9657\n",
      "test: 111/9657\n",
      "test: 112/9657\n",
      "test: 113/9657\n",
      "test: 114/9657\n",
      "test: 115/9657\n",
      "test: 116/9657\n",
      "test: 117/9657\n",
      "test: 118/9657\n",
      "test: 119/9657\n",
      "test: 120/9657\n",
      "test: 121/9657\n",
      "test: 122/9657\n",
      "test: 123/9657\n",
      "test: 124/9657\n",
      "test: 125/9657\n",
      "test: 126/9657\n",
      "test: 127/9657\n",
      "test: 128/9657\n",
      "test: 129/9657\n",
      "test: 130/9657\n",
      "test: 131/9657\n",
      "test: 132/9657\n",
      "test: 133/9657\n",
      "test: 134/9657\n",
      "test: 135/9657\n",
      "test: 136/9657\n",
      "test: 137/9657\n",
      "test: 138/9657\n",
      "test: 139/9657\n",
      "test: 140/9657\n",
      "test: 141/9657\n",
      "test: 142/9657\n",
      "test: 143/9657\n",
      "test: 144/9657\n",
      "test: 145/9657\n",
      "test: 146/9657\n",
      "test: 147/9657\n",
      "test: 148/9657\n",
      "test: 149/9657\n",
      "test: 150/9657\n",
      "test: 151/9657\n",
      "test: 152/9657\n",
      "test: 153/9657\n",
      "test: 154/9657\n",
      "test: 155/9657\n",
      "test: 156/9657\n",
      "test: 157/9657\n",
      "test: 158/9657\n",
      "test: 159/9657\n",
      "test: 160/9657\n",
      "test: 161/9657\n",
      "test: 162/9657\n",
      "test: 163/9657\n",
      "test: 164/9657\n",
      "test: 165/9657\n",
      "test: 166/9657\n",
      "test: 167/9657\n",
      "test: 168/9657\n",
      "test: 169/9657\n",
      "test: 170/9657\n",
      "test: 171/9657\n",
      "test: 172/9657\n",
      "test: 173/9657\n",
      "test: 174/9657\n",
      "test: 175/9657\n",
      "test: 176/9657\n",
      "test: 177/9657\n",
      "test: 178/9657\n",
      "test: 179/9657\n",
      "test: 180/9657\n",
      "test: 181/9657\n",
      "test: 182/9657\n",
      "test: 183/9657\n",
      "test: 184/9657\n",
      "test: 185/9657\n",
      "test: 186/9657\n",
      "test: 187/9657\n",
      "test: 188/9657\n",
      "test: 189/9657\n",
      "test: 190/9657\n",
      "test: 191/9657\n",
      "test: 192/9657\n",
      "test: 193/9657\n",
      "test: 194/9657\n",
      "test: 195/9657\n",
      "test: 196/9657\n",
      "test: 197/9657\n",
      "test: 198/9657\n",
      "test: 199/9657\n",
      "test: 200/9657\n",
      "test: 201/9657\n",
      "test: 202/9657\n",
      "test: 203/9657\n",
      "test: 204/9657\n",
      "test: 205/9657\n",
      "test: 206/9657\n",
      "test: 207/9657\n",
      "test: 208/9657\n",
      "test: 209/9657\n",
      "test: 210/9657\n",
      "test: 211/9657\n",
      "test: 212/9657\n",
      "test: 213/9657\n",
      "test: 214/9657\n",
      "test: 215/9657\n",
      "test: 216/9657\n",
      "test: 217/9657\n",
      "test: 218/9657\n",
      "test: 219/9657\n",
      "test: 220/9657\n",
      "test: 221/9657\n",
      "test: 222/9657\n",
      "test: 223/9657\n",
      "test: 224/9657\n",
      "test: 225/9657\n",
      "test: 226/9657\n",
      "test: 227/9657\n",
      "test: 228/9657\n",
      "test: 229/9657\n",
      "test: 230/9657\n",
      "test: 231/9657\n",
      "test: 232/9657\n",
      "test: 233/9657\n",
      "test: 234/9657\n",
      "test: 235/9657\n",
      "test: 236/9657\n",
      "test: 237/9657\n",
      "test: 238/9657\n",
      "test: 239/9657\n",
      "test: 240/9657\n",
      "test: 241/9657\n",
      "test: 242/9657\n",
      "test: 243/9657\n",
      "test: 244/9657\n",
      "test: 245/9657\n",
      "test: 246/9657\n",
      "test: 247/9657\n",
      "test: 248/9657\n",
      "test: 249/9657\n",
      "test: 250/9657\n",
      "test: 251/9657\n",
      "test: 252/9657\n",
      "test: 253/9657\n",
      "test: 254/9657\n",
      "test: 255/9657\n",
      "test: 256/9657\n",
      "test: 257/9657\n",
      "test: 258/9657\n",
      "test: 259/9657\n",
      "test: 260/9657\n",
      "test: 261/9657\n",
      "test: 262/9657\n",
      "test: 263/9657\n",
      "test: 264/9657\n",
      "test: 265/9657\n",
      "test: 266/9657\n",
      "test: 267/9657\n",
      "test: 268/9657\n",
      "test: 269/9657\n",
      "test: 270/9657\n",
      "test: 271/9657\n",
      "test: 272/9657\n",
      "test: 273/9657\n",
      "test: 274/9657\n",
      "test: 275/9657\n",
      "test: 276/9657\n",
      "test: 277/9657\n",
      "test: 278/9657\n",
      "test: 279/9657\n",
      "test: 280/9657\n",
      "test: 281/9657\n",
      "test: 282/9657\n",
      "test: 283/9657\n",
      "test: 284/9657\n",
      "test: 285/9657\n",
      "test: 286/9657\n",
      "test: 287/9657\n",
      "test: 288/9657\n",
      "test: 289/9657\n",
      "test: 290/9657\n",
      "test: 291/9657\n",
      "test: 292/9657\n",
      "test: 293/9657\n",
      "test: 294/9657\n",
      "test: 295/9657\n",
      "test: 296/9657\n",
      "test: 297/9657\n",
      "test: 298/9657\n",
      "test: 299/9657\n",
      "test: 300/9657\n",
      "test: 301/9657\n",
      "test: 302/9657\n",
      "test: 303/9657\n",
      "test: 304/9657\n",
      "test: 305/9657\n",
      "test: 306/9657\n",
      "test: 307/9657\n",
      "test: 308/9657\n",
      "test: 309/9657\n",
      "test: 310/9657\n",
      "test: 311/9657\n",
      "test: 312/9657\n",
      "test: 313/9657\n",
      "test: 314/9657\n",
      "test: 315/9657\n",
      "test: 316/9657\n",
      "test: 317/9657\n",
      "test: 318/9657\n",
      "test: 319/9657\n",
      "test: 320/9657\n",
      "test: 321/9657\n",
      "test: 322/9657\n",
      "test: 323/9657\n",
      "test: 324/9657\n",
      "test: 325/9657\n",
      "test: 326/9657\n",
      "test: 327/9657\n",
      "test: 328/9657\n",
      "test: 329/9657\n",
      "test: 330/9657\n",
      "test: 331/9657\n",
      "test: 332/9657\n",
      "test: 333/9657\n",
      "test: 334/9657\n",
      "test: 335/9657\n",
      "test: 336/9657\n",
      "test: 337/9657\n",
      "test: 338/9657\n",
      "test: 339/9657\n",
      "test: 340/9657\n",
      "test: 341/9657\n",
      "test: 342/9657\n",
      "test: 343/9657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 344/9657\n",
      "test: 345/9657\n",
      "test: 346/9657\n",
      "test: 347/9657\n",
      "test: 348/9657\n",
      "test: 349/9657\n",
      "test: 350/9657\n",
      "test: 351/9657\n",
      "test: 352/9657\n",
      "test: 353/9657\n",
      "test: 354/9657\n",
      "test: 355/9657\n",
      "test: 356/9657\n",
      "test: 357/9657\n",
      "test: 358/9657\n",
      "test: 359/9657\n",
      "test: 360/9657\n",
      "test: 361/9657\n",
      "test: 362/9657\n",
      "test: 363/9657\n",
      "test: 364/9657\n",
      "test: 365/9657\n",
      "test: 366/9657\n",
      "test: 367/9657\n",
      "test: 368/9657\n",
      "test: 369/9657\n",
      "test: 370/9657\n",
      "test: 371/9657\n",
      "test: 372/9657\n",
      "test: 373/9657\n",
      "test: 374/9657\n",
      "test: 375/9657\n",
      "test: 376/9657\n",
      "test: 377/9657\n",
      "test: 378/9657\n",
      "test: 379/9657\n",
      "test: 380/9657\n",
      "test: 381/9657\n",
      "test: 382/9657\n",
      "test: 383/9657\n",
      "test: 384/9657\n",
      "test: 385/9657\n",
      "test: 386/9657\n",
      "test: 387/9657\n",
      "test: 388/9657\n",
      "test: 389/9657\n",
      "test: 390/9657\n",
      "test: 391/9657\n",
      "test: 392/9657\n",
      "test: 393/9657\n",
      "test: 394/9657\n",
      "test: 395/9657\n",
      "test: 396/9657\n",
      "test: 397/9657\n",
      "test: 398/9657\n",
      "test: 399/9657\n",
      "test: 400/9657\n",
      "test: 401/9657\n",
      "test: 402/9657\n",
      "test: 403/9657\n",
      "test: 404/9657\n",
      "test: 405/9657\n",
      "test: 406/9657\n",
      "test: 407/9657\n",
      "test: 408/9657\n",
      "test: 409/9657\n",
      "test: 410/9657\n",
      "test: 411/9657\n",
      "test: 412/9657\n",
      "test: 413/9657\n",
      "test: 414/9657\n",
      "test: 415/9657\n",
      "test: 416/9657\n",
      "test: 417/9657\n",
      "test: 418/9657\n",
      "test: 419/9657\n",
      "test: 420/9657\n",
      "test: 421/9657\n",
      "test: 422/9657\n",
      "test: 423/9657\n",
      "test: 424/9657\n",
      "test: 425/9657\n",
      "test: 426/9657\n",
      "test: 427/9657\n",
      "test: 428/9657\n",
      "test: 429/9657\n",
      "test: 430/9657\n",
      "test: 431/9657\n",
      "test: 432/9657\n",
      "test: 433/9657\n",
      "test: 434/9657\n",
      "test: 435/9657\n",
      "test: 436/9657\n",
      "test: 437/9657\n",
      "test: 438/9657\n",
      "test: 439/9657\n",
      "test: 440/9657\n",
      "test: 441/9657\n",
      "test: 442/9657\n",
      "test: 443/9657\n",
      "test: 444/9657\n",
      "test: 445/9657\n",
      "test: 446/9657\n",
      "test: 447/9657\n",
      "test: 448/9657\n",
      "test: 449/9657\n",
      "test: 450/9657\n",
      "test: 451/9657\n",
      "test: 452/9657\n",
      "test: 453/9657\n",
      "test: 454/9657\n",
      "test: 455/9657\n",
      "test: 456/9657\n",
      "test: 457/9657\n",
      "test: 458/9657\n",
      "test: 459/9657\n",
      "test: 460/9657\n",
      "test: 461/9657\n",
      "test: 462/9657\n",
      "test: 463/9657\n",
      "test: 464/9657\n",
      "test: 465/9657\n",
      "test: 466/9657\n",
      "test: 467/9657\n",
      "test: 468/9657\n",
      "test: 469/9657\n",
      "test: 470/9657\n",
      "test: 471/9657\n",
      "test: 472/9657\n",
      "test: 473/9657\n",
      "test: 474/9657\n",
      "test: 475/9657\n",
      "test: 476/9657\n",
      "test: 477/9657\n",
      "test: 478/9657\n",
      "test: 479/9657\n",
      "test: 480/9657\n",
      "test: 481/9657\n",
      "test: 482/9657\n",
      "test: 483/9657\n",
      "test: 484/9657\n",
      "test: 485/9657\n",
      "test: 486/9657\n",
      "test: 487/9657\n",
      "test: 488/9657\n",
      "test: 489/9657\n",
      "test: 490/9657\n",
      "test: 491/9657\n",
      "test: 492/9657\n",
      "test: 493/9657\n",
      "test: 494/9657\n",
      "test: 495/9657\n",
      "test: 496/9657\n",
      "test: 497/9657\n",
      "test: 498/9657\n",
      "test: 499/9657\n",
      "test: 500/9657\n",
      "test: 501/9657\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b16d8e766606>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test: {}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mpredicted_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_test_seq_trunc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu_test_np\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0my_total\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_np\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mpred_total\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-b16d8e766606>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[1;34m(input_seq, user_info_input)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHOURS\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate_value\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m    \u001b[1;31m# this is ln(lambda)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmax_retweet_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mRETWEETS_NORM_TO\u001b[0m   \u001b[1;31m# unnormalize number of retweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zhuyl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1623\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1624\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1625\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Single epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1626\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1627\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zhuyl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     \u001b[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m       \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zhuyl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32mc:\\users\\zhuyl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    680\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zhuyl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    703\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m--> 705\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    706\u001b[0m       \u001b[1;31m# Delete the resource when this object is deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[1;32mc:\\users\\zhuyl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   2969\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2970\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m-> 2971\u001b[1;33m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[0;32m   2972\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2973\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# testing from here\n",
    "encoder_model = Model([encoder_inputs, user_info_inputs], encoder_output_dense1)\n",
    "decoder_state_input = Input(shape=(128,),name='input_4')\n",
    "decoder_outputs, decoder_state = decoder_rnn(decoder_inputs, initial_state=decoder_state_input)\n",
    "decoder_outputs = time_distributed(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs, decoder_state_input], [decoder_outputs] + [decoder_state])\n",
    "encoder_model.summary()\n",
    "decoder_model.summary()\n",
    "\n",
    "def decode_sequence(input_seq,user_info_input):\n",
    "    state_value = encoder_model.predict([input_seq, user_info_input])\n",
    "    \n",
    "    target = 0\n",
    "    target_list = []\n",
    "    for t in range(1,HOURS+1):\n",
    "        targets = np.array([[[target]]])\n",
    "        targets, state_value = decoder_model.predict([targets,state_value])\n",
    "        target = targets[0][0][0]    # this is ln(lambda)\n",
    "        target = (math.exp(target)*max_retweet_count)/RETWEETS_NORM_TO   # unnormalize number of retweets\n",
    "        # target = math.exp(target)    # this is lambda\n",
    "        target = max(math.ceil(target)-1, math.floor(target))\n",
    "        target_list.append(target)\n",
    "        target = (target/max_retweet_count)*RETWEETS_NORM_TO   # normalize number of retweets\n",
    "\n",
    "    return target_list  \n",
    "\n",
    "u_test_np = u_test.to_numpy()\t# this is input, hence its normalized\n",
    "y_test_np = y_test.to_numpy()*max_retweet_count/RETWEETS_NORM_TO   # GT\n",
    "test_size = len(X_test_seq_trunc)\n",
    "results = open('results.txt', 'w')\n",
    "y_total=[]\n",
    "pred_total=[]\n",
    "for i in range(test_size):\n",
    "    print('test: {}/{}'.format(i+1,test_size))\n",
    "    predicted_y = decode_sequence(np.array([X_test_seq_trunc[i]]),np.array([u_test_np[:][i]]))\n",
    "    y_total.append(y_test_np[i][-1])\n",
    "    pred_total.append(predicted_y[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(list(range(len(y_total))), y_total)\n",
    "plt.scatter(list(range(len(pred_total))), pred_total)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "print(pred_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
